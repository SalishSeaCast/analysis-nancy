{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Development of Live Ocean interpolation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from scipy import interpolate\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from salishsea_tools import viz_tools\n",
    "import grid\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions\n",
    "\n",
    "Some helper funcions for the interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I would prefer to do this with xarray but I'm having issues with units in the boundary files. \n",
    "def load_SalishSea_boundary_grid(\n",
    "    fname='/data/nsoontie/MEOPAR/NEMO-forcing/open_boundaries/west/SalishSea2_Masson_corrected.nc'\n",
    "):\n",
    "    \"\"\"Load the Salish Sea NEMO model boundary depth, latitudes and longitudes:\n",
    "    \n",
    "    :arg fname: name of boundary file\n",
    "    :type fname: str\n",
    "    \n",
    "    :returns: numpy arrays depth, lon, lat and a tuple shape\n",
    "    \"\"\"\n",
    "    \n",
    "    f = nc.Dataset(fname)\n",
    "    depth = f.variables['deptht'][:]\n",
    "    lon = f.variables['nav_lon'][:]\n",
    "    lat = f.variables['nav_lat'][:]\n",
    "    # determine lateral shape of boundary because arrays are flattened\n",
    "    width = f.variables['nbrdta'][:].flatten()[-1] + 1\n",
    "    length = int(f.variables['nbrdta'][:].shape[-1]/width)  \n",
    "    # Should I through an error if\n",
    "    shape = (width, length)\n",
    "    \n",
    "    return depth, lon, lat, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_LiveOcean(files, resample_interval = '1H'):\n",
    "    \"\"\"Load a time series of Live Ocean results represented by a list of files. \n",
    "    Time series is resampled by averaging over resample_interval.  Default is 1 hour.\n",
    "    \n",
    "    :arg files: Live Ocean filenames\n",
    "    :type files: list of strings\n",
    "    \n",
    "    :arg resample_interval: interval for resampling based on pandas values.\n",
    "                            e.g. 1H is one hour, 7D is seven days, etc\n",
    "    :type resample_interval: str\n",
    "    \n",
    "    :returns: xarray dataset of live ocean results\n",
    "    \"\"\"\n",
    "    # Loop through files and load\n",
    "    d = xr.open_dataset(files[0])\n",
    "    for f in files[1:]:\n",
    "        with xr.open_dataset(f) as d1:\n",
    "            d =xr.concat([d, d1], dim='ocean_time', data_vars='minimal')\n",
    "    # Determine z-rho (depth)\n",
    "    G, S, T = grid.get_basic_info(files[0])  # note: grid.py is a module from Parker. \n",
    "    z_rho = np.zeros(d.salt.shape)\n",
    "    for t in np.arange(z_rho.shape[0]):\n",
    "        zeta = d.zeta.values[t, : , :]\n",
    "        z_rho[t, :, :, :] = grid.get_z(G['h'], zeta, S)\n",
    "    # Add z_rho to dataset\n",
    "    zrho_DA = xr.DataArray(z_rho, dims = ['ocean_time', 's_rho', 'eta_rho', 'xi_rho'],\n",
    "                           coords = {'ocean_time': d.ocean_time.values[:],\n",
    "                                     's_rho': d.s_rho.values[:],\n",
    "                                     'eta_rho': d.eta_rho.values[:],\n",
    "                                     'xi_rho': d.xi_rho.values[:]},\n",
    "                           attrs = {'units': 'metres',\n",
    "                                    'positive': 'up',\n",
    "                                    'long_name': 'Depth at s-levels',\n",
    "                                    'field': 'z_rho ,scalar'})\n",
    "    d = d.assign(z_rho = zrho_DA)\n",
    "    # Resample\n",
    "    d = d.resample(resample_interval, 'ocean_time')\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolate_to_NEMO_depths(dataset, NEMO_depths, var_names):\n",
    "    \"\"\"Interpolate variables in var_names from a Live Ocean dataset to NEMO depths.\n",
    "    LiveOcean land points (including points lower than bathymetry) are set to np.nan and then masked. \n",
    "    \n",
    "    :arg dataset: Live Ocean dataset\n",
    "    :type dataset: xarray Dataset\n",
    "    \n",
    "    :arg NEMO_depths: NEMO model depths\n",
    "    :type NEMO_depths: 1D numpy array\n",
    "    \n",
    "    :arg var_names: list of Live Ocean variable names to be interpolated, e.g ['salt', 'temp']\n",
    "    :type var_names: list of str\n",
    "    \n",
    "    :returns: dictionary continaing interpolated numpy arrays for each variable\n",
    "    \"\"\"\n",
    "    interps = {}\n",
    "    for var_name in var_names:\n",
    "        var_interp = np.zeros(dataset[var_name].shape)\n",
    "        for t in np.arange(var_interp.shape[0]):\n",
    "            for j in np.arange(var_interp.shape[2]):\n",
    "                for i in np.arange(var_interp.shape[3]):\n",
    "                    LO_depths = dataset.z_rho.values[t, :, j, i] \n",
    "                    var = dataset[var_name].values[t, :, j, i]\n",
    "                    var_interp[t, :, j, i] = np.interp(-NEMO_depths, LO_depths, var, left=np.nan)\n",
    "                    # NEMO depths are positive, LiveOcean are negative\n",
    "        interps[var_name] = np.ma.masked_invalid(var_interp)\n",
    "    \n",
    "    return interps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_NaNs_with_nearest_neighbour(data, lons, lats):\n",
    "    \"\"\"At each depth level and time, fill in NaN values with nearest lateral neighbour. \n",
    "    If the entire depth level is NaN, fill with values from level above. \n",
    "    The last two dimensions of data are the lateral dimensions. \n",
    "    lons.shape and lats.shape = (data.shape[-2], data.shape[-1])\n",
    "    \n",
    "    :arg data: the data to be filled\n",
    "    :type data: 4D numpy array \n",
    "    \n",
    "    :arg lons: longitude points\n",
    "    :type lons: 2D numpy array\n",
    "    \n",
    "    :arg lats: latitude points\n",
    "    :type lats: 2D numpy array\n",
    "    \n",
    "    :returns: a 4D numpy array\n",
    "    \"\"\"\n",
    "    filled = data.copy()\n",
    "    for t in np.arange(data.shape[0]):\n",
    "        for k in np.arange(data.shape[1]):\n",
    "            subdata = data[t, k, :, :]\n",
    "            mask = np.isnan(subdata)\n",
    "            points = np.array([lons[~mask], lats[~mask]]).T\n",
    "            valid_data =subdata[~mask]\n",
    "            try:\n",
    "                filled[t,k,mask] = interpolate.griddata(points, valid_data, (lons[mask], lats[mask]),\n",
    "                                                        method='nearest')\n",
    "            except ValueError:  # if the whole depth level is NaN, set it equal to the level above\n",
    "                filled[t, k, :, :] = filled[t, k-1, :, :]\n",
    "    return filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolate_to_NEMO_lateral(var_arrays, dataset, NEMOlon, NEMOlat, shape):\n",
    "    \"\"\"Interpolates arrays in var_arrays laterally to NEMO grid. \n",
    "    Assumes these arrays have already been interpolated vertically.\n",
    "    NaN values are set to nearest lateral neighbour. \n",
    "    If a vertical level is entirely NaNs, it is set equal to the level above. \n",
    "    \n",
    "    :arg var_arrays: dictionary of 4D numpy arrays. Key represents the variable name.\n",
    "    :type var_arrrays: dictionary\n",
    "    \n",
    "    :arg dataset: LiveOcean results. Used to look up lateral grid.\n",
    "    :type dataset: xarray Dataset\n",
    "    \n",
    "    :arg NEMOlon: array of NEMO boundary longitudes\n",
    "    :type NEMOlon: 1D numpy array\n",
    "    \n",
    "    :arg NEMOlat: array of NEMO boundary longitudes\n",
    "    :type NEMOlat: 1D numpy array\n",
    "    \n",
    "    :arg shape: the lateral shape of NEMO boundary area.\n",
    "    :type shape: 2-tuple\n",
    "    \n",
    "    :returns: a dictionary, like var_arrays, but with arrays replaced with interpolated values \n",
    "    \"\"\"\n",
    "    # reshape our NEMO BC grid to use griddata\n",
    "    longrid = NEMOlon.reshape(shape)\n",
    "    latgrid = NEMOlat.reshape(shape)\n",
    "    NEMO_points = (longrid, latgrid)\n",
    "    # combine LiveOcean grid to use griddata\n",
    "    LO_points = np.array([dataset.lon_rho.values[:].flatten(), \n",
    "                          dataset.lat_rho.values[:].flatten()]).T\n",
    "    #interpolate each variable\n",
    "    interps = {}\n",
    "    for var_name, var in var_arrays.items():\n",
    "        var_new = np.zeros((var.shape[0], var.shape[1], 1, shape[0]*shape[1]))\n",
    "        for t in np.arange(var_new.shape[0]):\n",
    "            for k in np.arange(var_new.shape[1]):\n",
    "                var_grid = var[t, k, :, :]\n",
    "                var_interp = interpolate.griddata(LO_points, var_grid.flatten(), NEMO_points )\n",
    "                var_new[t, k, 0, :] = var_interp.flatten()\n",
    "        interps[var_name] = fill_NaNs_with_nearest_neighbour(var_new, NEMOlon, NEMOlat)\n",
    "    \n",
    "    return interps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Load our boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "depBC, lonBC, latBC, shape = load_SalishSea_boundary_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Live Ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = '/ocean/nsoontie/MEOPAR/LiveOcean/*/*UBC.nc'\n",
    "files = glob.glob(base)\n",
    "files.sort()\n",
    "\n",
    "weekly = load_LiveOcean(files, resample_interval='1D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interpolate\n",
    "\n",
    "### a. Depths\n",
    "First, interpolate onto our model level depths. Points below Lieve Ocean bathymetry and on land are set to NaN and masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "depth_interps = interpolate_to_NEMO_depths(weekly, depBC, ['salt', 'temp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check depth interpolation by comparing a few Live Ocean slices to depth_interps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_depth_interp(dataset, interps, depBC, var_name, levels, t, s, k, sl_val, sl_dir,\n",
    "                          lims = {'lat': [48.3,48.8], 'lon': [-125.1,-124.4]}):\n",
    "    \"\"\"This function is not part of the algorithm for creating boundary conditions. \n",
    "    It is only a quick function written for comapring results. \n",
    "    It does not need to be carefully reviewed\"\"\"\n",
    "    # Live Ocean Bathymetry\n",
    "    fname = '/ocean/nsoontie/MEOPAR/LiveOcean/20160601/ocean_his_0002_UBC.nc'\n",
    "    G, S, T = grid.get_basic_info(files[0])\n",
    "    # plottinh\n",
    "    fig,axs = plt.subplots(1,2,figsize=(15,3))\n",
    "    # interpolated\n",
    "    ax=axs[0]\n",
    "    if sl_dir == 'lat':\n",
    "        mesh = ax.contourf(dataset.lat_rho.values[:,sl_val], -depBC, interps[var_name][t,:,:,sl_val],\n",
    "                            levels, cmap='viridis')\n",
    "        ax.plot(dataset.lat_rho.values[:,sl_val],-G['h'][:,sl_val],'k-')\n",
    "    else:\n",
    "        mesh = ax.pcolormesh(dataset.lon_rho.values[sl_val,:], -depBC, interps[var_name][t,:,sl_val,:],\n",
    "                             vmin=clims[0],vmax=clims[1], cmap='viridis')\n",
    "        ax.plot(dataset.lon_rho.values[sl_val,:],-G['h'][sl_val,:],'k-')\n",
    "    cbar=plt.colorbar(mesh,ax=ax)\n",
    "    cbar.set_label(var_name)\n",
    "    ax.set_title('interpolated')\n",
    "    #orginal Live Ocean\n",
    "    ax=axs[1]\n",
    "    if sl_dir == 'lat':\n",
    "        plotting = dataset.isel(xi_rho=sl_val,ocean_time=t)\n",
    "        dplot = plotting.z_rho.values[:]\n",
    "        xplot,_ = np.meshgrid(plotting.lat_rho.values[:], dplot[:,0])\n",
    "    else:\n",
    "        plotting = dataset.isel(eta_rho=sl_val,ocean_time=t)\n",
    "        dplot = plotting.z_rho.values[:]\n",
    "        xplot,_ = np.meshgrid(plotting.lon_rho.values[:], dplot[:,0])\n",
    "    mesh=ax.contourf(xplot, dplot, plotting[var_name].values[:],\n",
    "                     levels, cmap='viridis')\n",
    "    cbar=plt.colorbar(mesh,ax=ax)\n",
    "    cbar.set_label(var_name)\n",
    "    ax.set_title('original')\n",
    "    for ax in axs:\n",
    "        ax.set_ylabel('Depth [m]')\n",
    "        ax.set_ylim([-450,0])\n",
    "        ax.set_xlim(lims[sl_dir])\n",
    "                       \n",
    "    fig,axs=plt.subplots(1,2,figsize=(15,3))\n",
    "    ax=axs[0]\n",
    "    mesh=ax.pcolormesh(dataset.lon_rho.values[:], dataset.lat_rho.values[:], interps[var_name][t,k,:,:],\n",
    "                       vmin=clims[0],vmax=clims[1], cmap='viridis')\n",
    "    ax.set_title('interpolated at depth={0:.3g} m'.format(depBC[k]))\n",
    "    cbar=plt.colorbar(mesh,ax=ax)\n",
    "    cbar.set_label(var_name)\n",
    "    ax=axs[1]\n",
    "    dataset.isel(s_rho=s, ocean_time=t)[var_name].plot(x='lon_rho', y='lat_rho',ax=ax,\n",
    "                                                       vmin=clims[0],vmax=clims[1], cmap='viridis')\n",
    "    ax.set_title('original at s_rho {}'.format(dataset.s_rho.values[s]))\n",
    "    for ax in axs:\n",
    "        if sl_dir == 'lat':\n",
    "            ax.plot(dataset.lon_rho.values[:,sl_val], dataset.lat_rho.values[:,sl_val],'k-')\n",
    "        else:\n",
    "            ax.plot(dataset.lon_rho.values[sl_val,:], dataset.lat_rho.values[sl_val,:],'k-')\n",
    "        ax.set_xlabel('long')\n",
    "        ax.set_ylabel('lat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sl_dir = 'lat'\n",
    "sl_val=13\n",
    "t=3\n",
    "s=39; k=0 #these are the values for looking at the surface. It is hard to comapre other depths.\n",
    "clims=(5, 15)\n",
    "levels=np.arange(5,15,1)\n",
    "var_name='temp'\n",
    "compare_depth_interp(weekly, depth_interps, depBC, var_name, levels, t, s, k, sl_val, sl_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clims=(29, 34)\n",
    "levels=np.arange(29,34.2,.2)\n",
    "var_name='salt'\n",
    "compare_depth_interp(weekly, depth_interps, depBC, var_name, levels, t, s, k, sl_val, sl_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These comparisons look reasonable.\n",
    "\n",
    "### b. Lateral\n",
    "\n",
    "Next, interpolate laterally using griddata. Fill NaNs with nearest neighbour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lateral_interps = interpolate_to_NEMO_lateral(depth_interps, weekly, lonBC, latBC, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparisons with depth interpolated Live Ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_lateral_interp(d_interps, dataset, l_interps, depths, lons, lats, shape, \n",
    "                           var_name, clims, t, k):\n",
    "    \"\"\"This function is not part of the algorithm for creating boundary conditions. \n",
    "    It is only a quick function written for comapring results. \n",
    "    It does not need to be carefully reviewed\"\"\"\n",
    "    fig,axs = plt.subplots(1,2,figsize=(15,3))\n",
    "    #Lateral interp\n",
    "    ax=axs[0]\n",
    "    longrid = lons.reshape(shape)\n",
    "    latgrid = lats.reshape(shape)\n",
    "    mesh=ax.pcolormesh(longrid, latgrid,\n",
    "                       l_interps[var_name][t, k, :, :].reshape(shape),\n",
    "                       vmin=clims[0], vmax=clims[1],cmap='viridis')\n",
    "    cbar = plt.colorbar(mesh,ax=ax)\n",
    "    cbar.set_label(var_name)\n",
    "    ax.set_title('Laterally interpolated: {0:.3f} m'.format(depths[k]))\n",
    "    #original lateral grid\n",
    "    ax=axs[1]\n",
    "    mesh=ax.pcolormesh(dataset.lon_rho.values[:], dataset.lat_rho.values[:],\n",
    "                       d_interps[var_name][t, k, :, :],\n",
    "                       vmin=clims[0], vmax=clims[1],cmap='viridis')\n",
    "    cbar=plt.colorbar(mesh,ax=ax)\n",
    "    cbar.set_label(var_name)\n",
    "    ax.set_title('original lateral grid: {0:.3f} m'.format(depths[k]))\n",
    "    ax.plot([longrid[0,0],longrid[-1,0]], [latgrid[0,0],latgrid[-1,0]], 'k-')\n",
    "    ax.plot([longrid[0,0],longrid[0,-1]], [latgrid[0,0],latgrid[0,-1]], 'k-')\n",
    "    ax.plot([longrid[-1,-1],longrid[-1,0]], [latgrid[-1,-1],latgrid[-1,0]], 'k-')\n",
    "    ax.plot([longrid[0,-1],longrid[-1,-1]], [latgrid[0,-1],latgrid[-1,-1]], 'k-')\n",
    "    #limits\n",
    "    for ax in axs:\n",
    "        ax.set_xlim([-125.1,-124.4])\n",
    "        ax.set_ylim([48.3,48.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t=0\n",
    "k=10\n",
    "clims=(5, 15)\n",
    "var_name='temp'\n",
    "compare_lateral_interp(depth_interps, weekly, lateral_interps, depBC, lonBC, latBC, shape,\n",
    "                       var_name, clims, t, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clims=(29, 34)\n",
    "var_name='salt'\n",
    "compare_lateral_interp(depth_interps, weekly, lateral_interps, depBC, lonBC, latBC, shape,\n",
    "                       var_name, clims, t, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this looks ok. We have also filled in land points with reasonable values. \n",
    "\n",
    "# Compare interpolated Live Ocean and  to current BCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load current BCs\n",
    "f = nc.Dataset('/data/nsoontie/MEOPAR/NEMO-forcing/open_boundaries/west/SalishSea2_Masson_corrected.nc')\n",
    "salBC = f.variables['vosaline'][:]\n",
    "tempBC = f.variables['votemper'][:]\n",
    "time = f.variables['time_counter'][:]\n",
    "#fake the dates\n",
    "start = datetime.datetime(2016,1,1)\n",
    "dates = [start + datetime.timedelta(weeks=int(d)) for d in time]\n",
    "b = nc.Dataset('/data/nsoontie/MEOPAR/NEMO-forcing/grid/bathy_meter_SalishSea2.nc')\n",
    "bathy = b.variables['Bathymetry'][:]\n",
    "nbjdta = f.variables['nbjdta'][:] \n",
    "nbidta = f.variables['nbidta'][:]\n",
    "\n",
    "bathyBC = bathy[nbjdta, nbidta]\n",
    "BC = {'salt': salBC, 'temp': tempBC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find time slices to compare\n",
    "t1=0\n",
    "t2=21\n",
    "ts = {'LO': t1, 'NEMO': t2}\n",
    "#check they are both first week of June\n",
    "print(weekly.ocean_time.values[t1],dates[t2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full BC comparison. First week of June."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare_BCs_full(NEMO_BC, LO_interp, depths, tims, bathy, var_name, levels):\n",
    "    fig,axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    #Live Ocean\n",
    "    ax=axs[0]\n",
    "    mesh=ax.contourf(np.arange(LO_interp[var_name].shape[-1]), depths,\n",
    "                    LO_interp[var_name][tims['LO'], :, 0, :],\n",
    "                    levels, cmap='viridis')\n",
    "    cbar = plt.colorbar(mesh,ax=ax)\n",
    "    cbar.set_label(var_name)\n",
    "    ax.set_title('Live Ocean')\n",
    "    #Our BCs\n",
    "    ax=axs[1]\n",
    "    mesh=ax.contourf(np.arange(NEMO_BC[var_name].shape[-1]), depths,\n",
    "                    NEMO_BC[var_name][tims['NEMO'], :, 0,:], \n",
    "                    levels, cmap='viridis')\n",
    "    cbar=plt.colorbar(mesh,ax=ax)\n",
    "    cbar.set_label(var_name)\n",
    "    ax.set_title('Our BCs')\n",
    "    #limits and bathy\n",
    "    for ax in axs:\n",
    "        ax.set_ylabel('Depth [m]')\n",
    "        ax.set_ylim(450,0)\n",
    "        ax.set_xlabel('boundary index')\n",
    "        ax.plot(np.arange(bathy.shape[1]), bathy[0,:], 'k-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "levels=np.arange(5,15,1)\n",
    "var_name='temp'\n",
    "compare_BCs_full(BC, lateral_interps, depBC, ts, bathyBC, var_name, levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Black lines are NEMO bathymetry. \n",
    "* Live Ocean is cooler below 100m (6-7 deg C vs 7-8 deg C). And warmer in surface.\n",
    "* A 'colder' current hugs the left boundary in our BCs (raised isotherms). Below 100m in Live Ocean the polarity of the isotherms on the left side reverses. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "levels=np.arange(31,34.2,.2)\n",
    "var_name='salt'\n",
    "compare_BCs_full(BC, lateral_interps, depBC, ts, bathyBC, var_name, levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Same polarity switch in the isohalines..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary\n",
    "* Given a list of Live Ocean files in chronological order, I  can interpolate onto our boundary with 4 function calls:\n",
    "    * depBC, lonBC, latBC, shape =  load_SalishSea_boundary()\n",
    "    * LO_dataset = load_LiveOcean(files, resample_interval='7D') - or choose a different sampling interval\n",
    "    * depth_interps = interpolate_to_NEMO_depths(LO_dataset, depBC, ['salt', 'temp'])\n",
    "    * lateral_interps = interpolate_to_NEMO_lateral(depth_interps, LO_dataset, lonBC, latBC, shape)\n",
    "\n",
    "# Next steps\n",
    "* For use in model runs, I need to figure out how to save as netCDF. Shouldn't be hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating boundary forcing files\n",
    "\n",
    "This set of functions generates boundary forcing files from Live Ocean results. One function does the trick, the others are helper functions\n",
    "\n",
    "For example, \n",
    "~~~~\n",
    "create_LiveOcean_TS_BCs('2016-06-01', '2016-07-31', '1D', 'monthly', \n",
    "                        basename = 'LO',\n",
    "                        save_dir='/ocean/nsoontie/MEOPAR/LiveOcean/boundary_files/', \n",
    "                        LO_dir='/ocean/nsoontie/MEOPAR/LiveOcean/',\n",
    "                        NEMO_BC='/data/nsoontie/MEOPAR/NEMO-forcing/open_boundaries/west/SalishSea2_Masson_corrected.nc'\n",
    "                        )\n",
    "~~~~\n",
    "\n",
    "creates forcing files for June 2016 and July 2016. The forcing files contained daily averaged fields. Two files are produced:\n",
    "* LO_y2016m06.nc\n",
    "* LO_y2016m07.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_LiveOcean_TS_BCs(start, end, avg_period, file_frequency, time_series = True,\n",
    "                            basename = 'LO',\n",
    "                            save_dir='/ocean/nsoontie/MEOPAR/LiveOcean/boundary_files/', \n",
    "                            LO_dir='/ocean/nsoontie/MEOPAR/LiveOcean/',\n",
    "                            NEMO_BC = '/data/nsoontie/MEOPAR/NEMO-forcing/open_boundaries/west/SalishSea2_Masson_corrected.nc'\n",
    "                            ):\n",
    "    \"\"\"Create a series of Live Ocean boundary condition files in date range [start, end]\n",
    "    for use in the NEMO model.\n",
    "    \n",
    "    :arg str start: start date in format 'yyyy-mm-dd'\n",
    "\n",
    "    :arg str end: end date in format 'yyyy-mm-dd\n",
    "    \n",
    "    :arg str avg_period: The averaging period for the forcing files.\n",
    "    options are '1H' for hourly, '1D' for daily, '7D' for weekly, '1M' for monthly\n",
    "    \n",
    "    :arg str file_frequency: The frequency by which the files will be saved.\n",
    "    Options are:\n",
    "    * 'yearly' files that contain a year of data and look like *_yYYYY.nc\n",
    "    * 'monthly' for files that contain a month of data and look like *_yYYYYmMM.nc\n",
    "    * 'daily' for files that contain a day of data and look like *_yYYYYmMMdDD.nc\n",
    "    where * is the basename.\n",
    "    \n",
    "    :arg time_series: Specifies that the boundary data is derived from a time series of LiveOcean\n",
    "    nowcast results. If false, the files are from a single 72 hour run beginning on start, in which case,\n",
    "    the argument end is ignored. \n",
    "    :type time_series: boolean\n",
    "    \n",
    "    :arg str basename: the base name of the saved files. \n",
    "    Eg. basename = 'LO', file_frequency = 'daily' saves files as 'LO_yYYYYmMMdDD.nc'\n",
    "    \n",
    "    :arg str save_dir: the directory in which to save the results\n",
    "    \n",
    "    :arg str LO_dir: the directory in which Live Ocean results are stored.\n",
    "    \n",
    "    :arg str NEMO_BC: path to an example NEMO boundary condition file for loading boundary info. \n",
    "    \"\"\"\n",
    "    # Create metadeta for temperature and salinity\n",
    "    var_meta = {'vosaline': {'grid': 'SalishSea2',\n",
    "                             'long_name': 'Practical Salinity',\n",
    "                             'units': 'psu'},\n",
    "                'votemper': {'grid': 'SalishSea2',\n",
    "                             'long_name': 'Potential Temperature',\n",
    "                             'units': 'deg C'}\n",
    "                } # We could pass a flag for conversion to TEOS-10, in which case metadata should be changed\n",
    "    \n",
    "    # Mapping from LiveOcean TS names to NEMO TS names\n",
    "    LO_to_NEMO_var_map = {'salt': 'vosaline',\n",
    "               'temp': 'votemper'}\n",
    "    \n",
    "    # Initialize var_arrays dict\n",
    "    NEMO_var_arrays = {key : [] for key in LO_to_NEMO_var_map.values()}\n",
    "    \n",
    "    # Load BC information\n",
    "    depBC, lonBC, latBC, shape =  load_SalishSea_boundary_grid(fname=NEMO_BC)\n",
    "    \n",
    "    # Load and interpolate Live Ocean\n",
    "    if time_series:\n",
    "        files = list_LO_time_series_files(start, end, LO_dir)\n",
    "    else: \n",
    "        print('Preparing 72 hours of Live Ocean results. Argument end = {} is ignored'.format(end))\n",
    "        files = list_LO_rundate_files(start, LO_dir)\n",
    "    LO_dataset = load_LiveOcean(files, resample_interval=avg_period)\n",
    "    depth_interps = interpolate_to_NEMO_depths(LO_dataset, depBC, ['salt', 'temp'])\n",
    "    lateral_interps = interpolate_to_NEMO_lateral(depth_interps, LO_dataset, lonBC, latBC, shape)\n",
    "    lateral_interps['ocean_time'] = LO_dataset.ocean_time\n",
    "    \n",
    "    # divide up data and save into separate files\n",
    "    separate_and_save_files(lateral_interps, save_dir, basename, file_frequency, avg_period, \n",
    "                            LO_to_NEMO_var_map, var_meta, NEMO_var_arrays, NEMO_BC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_LO_time_series_files(start, end, LO_dir):\n",
    "    \"\"\" List the Live Ocean files in a given date range [start, end].\n",
    "    LO nowcast files that form a time series are used.\n",
    "    Note: If start='2016-06-01' and end= '2016-06-02' results will be a list starting with\n",
    "    LO_dir/2016-05-31/ocean_his_0025_UBC.nc and ending with\n",
    "    LO_dir/2016-06-02/ocean_his_0024_UBC.nc.\n",
    "    The times in these files represent 2016-06-01 00:00:00 to 2016-06-02 23:00:00.\n",
    "    \n",
    "    :arg str start: start date in format 'yyyy-mm-dd'\n",
    "\n",
    "    :arg str end: end date in format 'yyyy-mm-dd\n",
    "    \n",
    "    :arg str LO_dir: the file path where Live Ocean results are stored\n",
    "    \n",
    "    :returns: list of Live Ocean file names\n",
    "    \"\"\"\n",
    "    \n",
    "    sdt = datetime.datetime.strptime(start,'%Y-%m-%d') - datetime.timedelta(days=1)\n",
    "    edt = datetime.datetime.strptime(end,'%Y-%m-%d')\n",
    "    sstr = os.path.join(LO_dir,'{}/ocean_his_0025_UBC.nc'.format(sdt.strftime('%Y%m%d')))\n",
    "    estr = os.path.join(LO_dir,'{}/ocean_his_0024_UBC.nc'.format(edt.strftime('%Y%m%d')))\n",
    "    \n",
    "    allfiles = glob.glob(os.path.join(LO_dir,'*/*UBC.nc'))\n",
    "    \n",
    "    files = []\n",
    "    for filename in allfiles:\n",
    "        if filename >=sstr and filename <= estr:\n",
    "            files.append(filename)\n",
    "\n",
    "    files.sort()\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_LO_rundate_files(rundate, LO_dir):\n",
    "    \"\"\" List 72 hours of Live Ocean files from a rundate.\n",
    "    Example: if rundate='2016-06-01'  the listed files will be\n",
    "    LO_dir/2016-05-31/ocean_his_0024_UBC.nc to LO_dir/2016-06-01/ocean_his_0072_UBC.nc\n",
    "    The times in these files represent 2016-06-01 00:00:00 to 2016-06-03 23:00:00.\n",
    "    \n",
    "    :arg str rundate: rundate in format 'yyyy-mm-dd'\n",
    "    \n",
    "    :arg str LO_dir: the file path where Live Ocean results are stored\n",
    "    \n",
    "    :returns: list of Live Ocean file names\n",
    "    \"\"\"\n",
    "    \n",
    "    sdt = datetime.datetime.strptime(rundate,'%Y-%m-%d')\n",
    "    files = glob.glob(os.path.join(LO_dir,sdt.strftime('%Y%m%d'), '*.nc'))\n",
    "    sdt_m1 = sdt - datetime.timedelta(days=1)\n",
    "    files.insert(0, os.path.join(LO_dir,sdt_m1.strftime('%Y%m%d'), 'ocean_his_0024_UBC.nc'))\n",
    "    files.sort()\n",
    "    del files[-1]\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_and_save_files(interpolated_data, save_dir, basename, file_frequency, avg_period, \n",
    "                            LO_to_NEMO_var_map, var_meta, NEMO_var_arrays, NEMO_BC_file):\n",
    "    # divide up data and save into separate files\n",
    "    time_units = {'1H': 'hours', '1D': 'days', '7D': 'weeks', '1M': 'months'}\n",
    "    index = 0\n",
    "    first = datetime.datetime.strptime(\n",
    "        str(interpolated_data['ocean_time'].values[0])[0:-3], '%Y-%m-%dT%H:%M:%S.%f'\n",
    "    )\n",
    "    # I don't really like method of retrieving the date from LO results. Is it necessary? .\n",
    "    first = first.replace(second=0, microsecond=0)\n",
    "    print(interpolated_data['ocean_time'])\n",
    "    for counter, t in enumerate(interpolated_data['ocean_time']):\n",
    "        date = datetime.datetime.strptime(str(t.values)[0:-3], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "        conditions ={'yearly':  date.year  != first.year, \n",
    "                     'monthly': date.month != first.month,  # doesn't work if same months, different year...\n",
    "                     'daily':   date.date() != first.date()\n",
    "                     }\n",
    "        filenames = {'yearly': os.path.join(save_dir,'{}_y{}.nc'.format(basename, first.year)),\n",
    "                     'monthly': os.path.join(save_dir,'{}_y{}m{:02d}.nc'.format(basename, first.year,\n",
    "                                                                                first.month)),\n",
    "                     'daily': os.path.join(save_dir,'{}_y{}m{:02d}d{:02d}.nc'.format(basename, first.year,\n",
    "                                                                                     first.month, first.day))\n",
    "                    }\n",
    "        if conditions[file_frequency]:\n",
    "                for LO_name, NEMO_name in LO_to_NEMO_var_map.items():\n",
    "                    var_arrays[NEMO_name] = interpolated_data[LO_name][index:counter,:,:,:]\n",
    "                create_sub_file(first, time_units[avg_period], \n",
    "                                NEMO_var_arrays, var_meta, NEMO_BC_file, filenames[file_frequency])\n",
    "                first = date\n",
    "                index = counter\n",
    "        elif counter == LO_dataset.ocean_time.values.shape[0]-1:\n",
    "                for LO_name, NEMO_name in LO_to_NEMO_var_map.items():\n",
    "                    var_arrays[NEMO_name] = interpolated_data[LO_name][index:,:,:,:]\n",
    "                create_sub_file(first, time_units[avg_period],\n",
    "                                NEMO_var_arrays, var_meta, NEMO_BC_file, filenames[file_frequency])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_sub_file(date, time_unit, var_arrays, var_meta,  NEMO_BC, filename):\n",
    "    \"\"\"Save a netCDF file for boundary data stored in var_arrays.\n",
    "    \n",
    "    :arg date: Date from which time in var_arrays is measured.\n",
    "    :type date: datetime object\n",
    "    \n",
    "    :arg str time_unit: Units that time in var_arrays is measured in. e.g 'days' or 'weeks' or 'hours'\n",
    "    \n",
    "    :arg var_arrays: a dictionary containing the boundary data to be saved.\n",
    "    :type var_arrays: dictionary of numpy arrays\n",
    "    \n",
    "    :arg var_meta: metadata for each variable in  var_arrays\n",
    "    :type var_meta: a dictionary of dictionaries with key-value pairs of metadata\n",
    "    \n",
    "    :arg str NEMO_BC: path to a current NEMO boundary file. Used for looping up boundary indices etc.\n",
    "    \n",
    "    :arg str filename: The name of the file to be saved.\n",
    "    \"\"\"    \n",
    "    # Set up xarray Dataset\n",
    "    ds = xr.Dataset()\n",
    "    \n",
    "    # Load BC information\n",
    "    f = nc.Dataset(NEMO_BC)\n",
    "    depBC = f.variables['deptht']\n",
    "    \n",
    "    # Copy variables and attributes of non-time dependent variables from a previous BC file\n",
    "    keys = list(f.variables.keys())\n",
    "    for var_name in var_arrays:\n",
    "        if var_name in keys: # check that var_name can be removed \n",
    "            keys.remove(var_name)\n",
    "    keys.remove('time_counter') # Allow xarray to build the arrays for these coordinates\n",
    "    keys.remove('deptht')\n",
    "    # Now iterate through remaining variables in old BC file and add to dataset\n",
    "    for key in keys:\n",
    "        var = f.variables[key]\n",
    "        temp_array = xr.DataArray(var,\n",
    "                                  name = key,\n",
    "                                  dims = list(var.dimensions),\n",
    "                                  attrs = {att: var.getncattr(att) for att in var.ncattrs()}\n",
    "                                 )\n",
    "        ds = xr.merge([ds, temp_array])\n",
    "    # Add better units information nbidta etc\n",
    "    for varname in ['nbidta', 'nbjdta', 'nbrdta']:\n",
    "        ds[varname].attrs['units'] = 'index'\n",
    "    # Now add the time-dependent model variables\n",
    "    for var_name, var_array in var_arrays.items():\n",
    "        data_array = xr.DataArray(var_array,\n",
    "                                  name = var_name,\n",
    "                                  dims = ['time_counter', 'deptht', 'yb', 'xbT'],\n",
    "                                  coords = {'deptht': (['deptht'], depBC[:]),\n",
    "                                            'time_counter': np.arange(var_array.shape[0])\n",
    "                                            }  ,\n",
    "                                  attrs = var_meta[var_name]\n",
    "                                 )\n",
    "        ds = xr.merge([ds, data_array])\n",
    "    # Fix metadata on time_counter\n",
    "    ds['time_counter'].attrs['units'] = '{} since {}'.format(time_unit, date.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    ds['time_counter'].attrs['time_origin'] = date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    ds['time_counter'].attrs['long_name'] = 'Time axis'\n",
    "    # Add metadata for deptht\n",
    "    ds['deptht'].attrs = {att: depBC.getncattr(att) for att in depBC.ncattrs()}\n",
    "    # Add some global attributes\n",
    "    ds.attrs = {'acknowledgements': 'Live Ocean http://faculty.washington.edu/pmacc/LO/LiveOcean.html',\n",
    "                'creator_email': 'nsoontie@eos.ubc.ca',\n",
    "                'creator_name': 'Salish Sea MEOPAR Project Contributors',\n",
    "                'creator_url': 'https://salishsea-meopar-docs.readthedocs.org/',\n",
    "                'institution': 'UBC EOAS',\n",
    "                'institution_fullname': 'Earth, Ocean & Atmospheric Sciences, University of British Columbia',\n",
    "                'summary': 'Temperature and Salinity from the Live Ocean model interpolated in space '\\\n",
    "                            'onto the Salish Sea NEMO Model western open boundary. ',\n",
    "                'source': 'http://nbviewer.jupyter.org/urls/bitbucket.org/salishsea/analysis-nancy/raw/tip/notebooks/'\\\n",
    "                          'LiveOcean/Interpolating%20Live%20Ocean%20to%20our%20boundary.ipynb',\n",
    "                'history': '[{}] File creation.'.format(datetime.datetime.today().strftime('%Y-%m-%d'))}\n",
    "    ds.to_netcdf(filename)\n",
    "    print('Saved {}'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing time series creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_LiveOcean_TS_BCs('2016-06-01', '2016-07-31', '1D', 'monthly', time_series=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing single run date creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_LiveOcean_TS_BCs('2016-06-09', '2016-06-09', '1H', 'daily', time_series=False,\n",
    "                       save_dir='/ocean/nsoontie/MEOPAR/LiveOcean/boundary_files/test',\n",
    "                       LO_dir = '/ocean/nsoontie/MEOPAR/LiveOcean/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Notes\n",
    "* Live Ocena files store salinity and temperature as Practical Salinity and Potential Temperature. I could add a convert_teos=true flag to create_LiveOcean_TS_boundary\n",
    "* I need to run \n",
    "~~~~ \n",
    "ncks --mk_rec_dmn=time_counter infile outfile \n",
    "~~~~ \n",
    "for these files to work in NEMO. Right now I can only run that on salish from the command line.\n",
    "* This depends on some python functions that Parker sent me for creating the depths and grid. Parker's funtions work with netCDF4 objects.\n",
    "    1. Where do we store Parker's code? ... private_tools or permission on a public repo? \n",
    "    2. Should we rework Parker's code to use xarray?\n",
    "* create_sub_file should be able to accommodate variables besides T+S. My suggestion is to create a new function similar to create_LiveOcean_TS_BCs. There is some repeated code that could be cleaned up.\n",
    "* I don't really like that some metadata and information is read from the old boundary files. Is that a bad choice?\n",
    "* I don't like that deptht is both a coordinate and a variable but this choice is most consistent with current BC files\n",
    "\n",
    "\n",
    "# LiveOcean_BCs.py\n",
    "The following functions have been moved into LiveOcean_BCs.py\n",
    "\n",
    "~~~~\n",
    "create_LiveOcean_TS_BCs()\n",
    "create_sub_file()\n",
    "list_LO_files_in_range()\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
